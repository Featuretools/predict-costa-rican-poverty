{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1a87dfa6b419deab71f1a128e3bf80a6a0b519ac"
   },
   "source": [
    "# Featuretools for Good\n",
    "\n",
    "In this notebook, we will implement automated feature engineering with [Featuretools](https://docs.featuretools.com/#minute-quick-start) for the Costa Rican Household Poverty Challenge. The objective of this data science for good problem is to predict the poverty of households in Costa Rica. \n",
    "\n",
    "## Automated Feature Engineering\n",
    "\n",
    "Automated feature engineering should be a _default_ part of your data science workflow. Manual feature engineering is limited both by human creativity and time constraints but automated methods have no such constraints. At the moment, Featuretools is the only open-source Python library available for automated feature engineering. This library is extremely easy to get started with and very powerful (as the score from this kernel illustrates). \n",
    "\n",
    "For anyone new to featuretools, check out the [documentation](https://docs.featuretools.com/getting_started/install.html) or an [introductory blog post here.](https://towardsdatascience.com/automated-feature-engineering-in-python-99baf11cc219) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "import featuretools as ft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e511a5ea8fff2f8ce3ac3a457eca2feeaacbb037"
   },
   "source": [
    "We'll read in the data and join the training and testing set together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# Raw data\n",
    "train = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test.csv')\n",
    "test['Target'] = np.nan\n",
    "\n",
    "data = train.append(test, sort = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6fbf5be29712683b8c385b867f2247e422ba7ece"
   },
   "source": [
    "### Data Preprocessing \n",
    "\n",
    "These steps are laid out in the kernel [Start Here: A Complete Walkthrough](https://www.kaggle.com/willkoehrsen/start-here-a-complete-walkthrough). Mostly we correct the labels, extract the labels for the heads of households, and establish a base dataframe for making submisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "80e06e60683d8e513992cce5fea4b089dbc3f1ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 85 households where the family members do not all have the same target.\n",
      "There are 0 households where the family members do not all have the same target.\n"
     ]
    }
   ],
   "source": [
    "# Groupby the household and figure out the number of unique values\n",
    "all_equal = train.groupby('idhogar')['Target'].apply(lambda x: x.nunique() == 1)\n",
    "\n",
    "# Households where targets are not all equal\n",
    "not_equal = all_equal[all_equal != True]\n",
    "print('There are {} households where the family members do not all have the same target.'.format(len(not_equal)))\n",
    "\n",
    "# Iterate through each household\n",
    "for household in not_equal.index:\n",
    "    # Find the correct label (for the head of household)\n",
    "    true_target = int(train[(train['idhogar'] == household) & (train['parentesco1'] == 1.0)]['Target'])\n",
    "    \n",
    "    # Set the correct label for all members in the household\n",
    "    train.loc[train['idhogar'] == household, 'Target'] = true_target\n",
    "    \n",
    "# Groupby the household and figure out the number of unique values\n",
    "all_equal = train.groupby('idhogar')['Target'].apply(lambda x: x.nunique() == 1)\n",
    "\n",
    "# Households where targets are not all equal\n",
    "not_equal = all_equal[all_equal != True]\n",
    "print('There are {} households where the family members do not all have the same target.'.format(len(not_equal)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8a3f5652300b83789c2fc01f9944be947233e3c9"
   },
   "source": [
    "Here we extract the train labels for the heads of households."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_valid = train.loc[train['parentesco1'] == 1, ['idhogar', 'Target']].copy()\n",
    "test_valid = test.loc[test['parentesco1'] == 1, ['idhogar', 'Target']].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fc8d9ea3fb4f554fb784a3e1019acd5e4bb5decc"
   },
   "source": [
    "We need to make predictions for _all individuals_ in the test data, but only the predictions for the heads of household are scored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "fd76091716fe5e2167369cceeba7aa08c15375b8"
   },
   "outputs": [],
   "source": [
    "submission_base = test.loc[:, ['idhogar', 'Id']]\n",
    "\n",
    "# The tests ids are only for the heads of households.\n",
    "# test_ids = list(test.loc[test['parentesco1'] == 1, 'idhogar'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a10b0ed8a9d6cb8acb43590c6ddc3e2904cf21bf"
   },
   "source": [
    "### Convert object types to floats\n",
    "\n",
    "The mapping is explained in the data description. These are continuous variables and should be converted to numeric floats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "808056c39087e5ef59c0131ee73e2a47b610f71e"
   },
   "outputs": [],
   "source": [
    "mapping = {\"yes\": 1, \"no\": 0}\n",
    "\n",
    "# Fill in the values with the correct mapping\n",
    "data['dependency'] = data['dependency'].replace(mapping).astype(np.float64)\n",
    "data['edjefa'] = data['edjefa'].replace(mapping).astype(np.float64)\n",
    "data['edjefe'] = data['edjefe'].replace(mapping).astype(np.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1dd27d18673153236e05c001dc60bec879a92d2e"
   },
   "source": [
    "### Handle Missing Values\n",
    "\n",
    "The logic for these choices is explained in the [Start Here: A Complete Walkthrough](https://www.kaggle.com/willkoehrsen/start-here-a-complete-walkthrough) kernel. This might not be optimal, but it has improved cross-validation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "2b64197ee2656a1d69c94e0c6ffd1c76dd91619b"
   },
   "outputs": [],
   "source": [
    "data['v18q1'] = data['v18q1'].fillna(0)\n",
    "data.loc[(data['tipovivi1'] == 1), 'v2a1'] = 0\n",
    "data['rez_esc'] = data['rez_esc'].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "17b36b4846334fcb78ecdd089564701d4b5b6efc"
   },
   "source": [
    "### Remove Squared Variables\n",
    "\n",
    "The gradient boosting machine does not need the squared version of variables it if already has the original variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "2c564ef909f8d1bf861a3cb8b573106a7297894a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33413, 134)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data[[x for x in data if not x.startswith('SQB')]]\n",
    "data = data.drop(columns = ['agesq'])\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "e29e93c437bd63a0e9f8b5596aa6040d1859984f"
   },
   "outputs": [],
   "source": [
    "import featuretools.variable_types as vtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7bfb23571103d16934da45554ba02d4f915e9519"
   },
   "source": [
    "#  Establish Correct Variable Types\n",
    "\n",
    "We need to specify the correct variables types:\n",
    "\n",
    "1. Individual Variables: these are characteristics of each individual rather than the household\n",
    "    * Boolean: Yes or No (0 or 1)\n",
    "    * Ordered Discrete: Integers with an ordering\n",
    "2. Household variables\n",
    "    * Boolean: Yes or No\n",
    "    * Ordered Discrete: Integers with an ordering\n",
    "    * Continuous numeric\n",
    "\n",
    "Below we manually define the variables in each category. This is a little tedious, but also necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "975463b8c6841e6894d6be8a733e29fb1386fcde"
   },
   "outputs": [],
   "source": [
    "hh_bool = ['hacdor', 'hacapo', 'v14a', 'refrig', 'paredblolad', 'paredzocalo', \n",
    "           'paredpreb','pisocemento', 'pareddes', 'paredmad',\n",
    "           'paredzinc', 'paredfibras', 'paredother', 'pisomoscer', 'pisoother', \n",
    "           'pisonatur', 'pisonotiene', 'pisomadera',\n",
    "           'techozinc', 'techoentrepiso', 'techocane', 'techootro', 'cielorazo', \n",
    "           'abastaguadentro', 'abastaguafuera', 'abastaguano',\n",
    "            'public', 'planpri', 'noelec', 'coopele', 'sanitario1', \n",
    "           'sanitario2', 'sanitario3', 'sanitario5',   'sanitario6',\n",
    "           'energcocinar1', 'energcocinar2', 'energcocinar3', 'energcocinar4', \n",
    "           'elimbasu1', 'elimbasu2', 'elimbasu3', 'elimbasu4', \n",
    "           'elimbasu5', 'elimbasu6', 'epared1', 'epared2', 'epared3',\n",
    "           'etecho1', 'etecho2', 'etecho3', 'eviv1', 'eviv2', 'eviv3', \n",
    "           'tipovivi1', 'tipovivi2', 'tipovivi3', 'tipovivi4', 'tipovivi5', \n",
    "           'computer', 'television', 'lugar1', 'lugar2', 'lugar3',\n",
    "           'lugar4', 'lugar5', 'lugar6', 'area1', 'area2']\n",
    "\n",
    "hh_ordered = [ 'rooms', 'r4h1', 'r4h2', 'r4h3', 'r4m1','r4m2','r4m3', 'r4t1',  'r4t2', \n",
    "              'r4t3', 'v18q1', 'tamhog','tamviv','hhsize','hogar_nin',\n",
    "              'hogar_adul','hogar_mayor','hogar_total',  'bedrooms', 'qmobilephone']\n",
    "\n",
    "hh_cont = ['v2a1', 'dependency', 'edjefe', 'edjefa', 'meaneduc', 'overcrowding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "4eff666cd77701b3787bc74c42f146b67219c070"
   },
   "outputs": [],
   "source": [
    "ind_bool = ['v18q', 'dis', 'male', 'female', 'estadocivil1', 'estadocivil2', 'estadocivil3', \n",
    "            'estadocivil4', 'estadocivil5', 'estadocivil6', 'estadocivil7', \n",
    "            'parentesco1', 'parentesco2',  'parentesco3', 'parentesco4', 'parentesco5', \n",
    "            'parentesco6', 'parentesco7', 'parentesco8',  'parentesco9', 'parentesco10', \n",
    "            'parentesco11', 'parentesco12', 'instlevel1', 'instlevel2', 'instlevel3', \n",
    "            'instlevel4', 'instlevel5', 'instlevel6', 'instlevel7', 'instlevel8', \n",
    "            'instlevel9', 'mobilephone']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0579f590db9153c4a68720e81b7a341cede4d009"
   },
   "source": [
    "Below we convert the `Boolean` variables to the correct type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "801e6565b54d6797c694b9a4666250203036fd2e"
   },
   "outputs": [],
   "source": [
    "for variable in (hh_bool + ind_bool):\n",
    "    data[variable] = data[variable].astype('bool')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c94d438dddaa1c2e08259a3a2ed01118173d328f"
   },
   "source": [
    "# EntitySet and Entities\n",
    "\n",
    "An `EntitySet` in Featuretools holds all of the tables and the relationships between them. At the moment we only have a single table, but we can create multiple tables through normalization. We'll call the first table `ind` since it contains information at the individual level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "63901e5cabcba3760dd711f9086d4023dca4eb7e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Entityset: households\n",
       "  Entities:\n",
       "    ind [Rows: 33413, Columns: 134]\n",
       "  Relationships:\n",
       "    No relationships"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es = ft.EntitySet(id = 'households')\n",
    "es.entity_from_dataframe(entity_id = 'ind', \n",
    "                         dataframe = data, \n",
    "                         index = 'Id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "78b1b739b016732e5ad504f92f632eea3ebd92ed"
   },
   "source": [
    "# Normalize Household Table\n",
    "\n",
    "Normalization allows us to create another table with one unique row per instance. In this case, the instances are households. The new table is derived from the `ind` table and we need to bring along any of the household level variables. Since these are the same for all members of a household, we can directly add these as columns in the household table using `additional_variables`. The index of the household table is `idhogar` which uniquely identifies each household.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "ab897f0260e7dda326a0d0ecdbf1c613c1bf6088"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Entityset: households\n",
       "  Entities:\n",
       "    ind [Rows: 33413, Columns: 38]\n",
       "    household [Rows: 10340, Columns: 97]\n",
       "  Relationships:\n",
       "    ind.idhogar -> household.idhogar"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es.normalize_entity(base_entity_id='ind', \n",
    "                    new_entity_id='household', \n",
    "                    index = 'idhogar', \n",
    "                    additional_variables = hh_bool + hh_ordered + hh_cont + ['Target'])\n",
    "es"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bb81ef01856ffe3d01bcf2ba6005dffda1c1836a"
   },
   "source": [
    "Normalizing the entity automatically adds in the relationship between the parent, `household`, and the child, `ind`. This relationship links the two tables and allows us to create \"deep features\" by aggregating individuals in each household."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0808dac4226963c7d2c78d8a39bf3eae949d56f1"
   },
   "source": [
    "# Deep Feature Synthesis\n",
    "\n",
    "Here is where Featuretools gets to work. Using feature primitives, Deep Feature Synthesis can build hundreds (or 1000s as we will later see) of features from the relationships between tables and the columns in tables themselves. There are two types of primitives, which are operations applied to data:\n",
    "\n",
    "* Transforms\n",
    "* Aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "755c363b6648af063d4d5a2b4c944952dc91c64d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built 148 features\n",
      "EntitySet scattered to workers in 3.043 seconds\n",
      "Elapsed: 01:21 | Remaining: 00:00 | Progress: 100%|██████████| Calculated: 104/104 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tornado.application - ERROR - Exception in Future <Future cancelled> after timeout\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/gaurav/.pyenv/versions/3.7.0/lib/python3.7/site-packages/tornado/gen.py\", line 970, in error_callback\n",
      "    future.result()\n",
      "concurrent.futures._base.CancelledError\n",
      "distributed.comm.tcp - WARNING - Closing dangling stream in <TCP local=tcp://127.0.0.1:51412 remote=tcp://127.0.0.1:51398>\n",
      "distributed.comm.tcp - WARNING - Closing dangling stream in <TCP local=tcp://127.0.0.1:51424 remote=tcp://127.0.0.1:51398>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hacdor</th>\n",
       "      <th>hacapo</th>\n",
       "      <th>v14a</th>\n",
       "      <th>refrig</th>\n",
       "      <th>paredblolad</th>\n",
       "      <th>paredzocalo</th>\n",
       "      <th>paredpreb</th>\n",
       "      <th>pisocemento</th>\n",
       "      <th>pareddes</th>\n",
       "      <th>paredmad</th>\n",
       "      <th>...</th>\n",
       "      <th>PERCENT_TRUE(ind.parentesco12)</th>\n",
       "      <th>PERCENT_TRUE(ind.parentesco2)</th>\n",
       "      <th>PERCENT_TRUE(ind.parentesco3)</th>\n",
       "      <th>PERCENT_TRUE(ind.parentesco4)</th>\n",
       "      <th>PERCENT_TRUE(ind.parentesco5)</th>\n",
       "      <th>PERCENT_TRUE(ind.parentesco6)</th>\n",
       "      <th>PERCENT_TRUE(ind.parentesco7)</th>\n",
       "      <th>PERCENT_TRUE(ind.parentesco8)</th>\n",
       "      <th>PERCENT_TRUE(ind.parentesco9)</th>\n",
       "      <th>PERCENT_TRUE(ind.v18q)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>idhogar</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>000a08204</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000bce7c4</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>001845fb0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>001ff74ca</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>003123ec2</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 148 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           hacdor  hacapo  v14a  refrig  paredblolad  paredzocalo  paredpreb  \\\n",
       "idhogar                                                                        \n",
       "000a08204   False   False  True    True         True        False      False   \n",
       "000bce7c4   False   False  True    True        False         True      False   \n",
       "001845fb0   False   False  True    True         True        False      False   \n",
       "001ff74ca   False   False  True    True         True        False      False   \n",
       "003123ec2   False   False  True    True        False        False       True   \n",
       "\n",
       "           pisocemento  pareddes  paredmad           ...            \\\n",
       "idhogar                                              ...             \n",
       "000a08204        False     False     False           ...             \n",
       "000bce7c4        False     False     False           ...             \n",
       "001845fb0        False     False     False           ...             \n",
       "001ff74ca        False     False     False           ...             \n",
       "003123ec2         True     False     False           ...             \n",
       "\n",
       "           PERCENT_TRUE(ind.parentesco12)  PERCENT_TRUE(ind.parentesco2)  \\\n",
       "idhogar                                                                    \n",
       "000a08204                             0.0                       0.333333   \n",
       "000bce7c4                             0.0                       0.500000   \n",
       "001845fb0                             0.0                       0.250000   \n",
       "001ff74ca                             0.0                       0.000000   \n",
       "003123ec2                             0.0                       0.250000   \n",
       "\n",
       "           PERCENT_TRUE(ind.parentesco3)  PERCENT_TRUE(ind.parentesco4)  \\\n",
       "idhogar                                                                   \n",
       "000a08204                       0.333333                            0.0   \n",
       "000bce7c4                       0.000000                            0.0   \n",
       "001845fb0                       0.500000                            0.0   \n",
       "001ff74ca                       0.500000                            0.0   \n",
       "003123ec2                       0.500000                            0.0   \n",
       "\n",
       "           PERCENT_TRUE(ind.parentesco5)  PERCENT_TRUE(ind.parentesco6)  \\\n",
       "idhogar                                                                   \n",
       "000a08204                            0.0                            0.0   \n",
       "000bce7c4                            0.0                            0.0   \n",
       "001845fb0                            0.0                            0.0   \n",
       "001ff74ca                            0.0                            0.0   \n",
       "003123ec2                            0.0                            0.0   \n",
       "\n",
       "           PERCENT_TRUE(ind.parentesco7)  PERCENT_TRUE(ind.parentesco8)  \\\n",
       "idhogar                                                                   \n",
       "000a08204                            0.0                            0.0   \n",
       "000bce7c4                            0.0                            0.0   \n",
       "001845fb0                            0.0                            0.0   \n",
       "001ff74ca                            0.0                            0.0   \n",
       "003123ec2                            0.0                            0.0   \n",
       "\n",
       "           PERCENT_TRUE(ind.parentesco9)  PERCENT_TRUE(ind.v18q)  \n",
       "idhogar                                                           \n",
       "000a08204                            0.0                     1.0  \n",
       "000bce7c4                            0.0                     0.0  \n",
       "001845fb0                            0.0                     0.0  \n",
       "001ff74ca                            0.0                     1.0  \n",
       "003123ec2                            0.0                     0.0  \n",
       "\n",
       "[5 rows x 148 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_matrix, feature_names = ft.dfs(entityset=es, \n",
    "                                       target_entity = 'household', \n",
    "                                       max_depth = 2, \n",
    "                                       verbose = 1, \n",
    "                                       n_jobs = -1, \n",
    "                                       chunk_size = 100)\n",
    "\n",
    "feature_matrix.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9154c4be3ba4b60868e1ae4eeef3bcd205fecdd9"
   },
   "source": [
    "We need to remove any columns containing derivations of the `Target`. These are created because some of transform primitives might have affected the `Target`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "6ad41824d9092f1ee2c494632dbab4a1d084f92a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hacdor</th>\n",
       "      <th>hacapo</th>\n",
       "      <th>v14a</th>\n",
       "      <th>refrig</th>\n",
       "      <th>paredblolad</th>\n",
       "      <th>paredzocalo</th>\n",
       "      <th>paredpreb</th>\n",
       "      <th>pisocemento</th>\n",
       "      <th>pareddes</th>\n",
       "      <th>paredmad</th>\n",
       "      <th>...</th>\n",
       "      <th>PERCENT_TRUE(ind.parentesco12)</th>\n",
       "      <th>PERCENT_TRUE(ind.parentesco2)</th>\n",
       "      <th>PERCENT_TRUE(ind.parentesco3)</th>\n",
       "      <th>PERCENT_TRUE(ind.parentesco4)</th>\n",
       "      <th>PERCENT_TRUE(ind.parentesco5)</th>\n",
       "      <th>PERCENT_TRUE(ind.parentesco6)</th>\n",
       "      <th>PERCENT_TRUE(ind.parentesco7)</th>\n",
       "      <th>PERCENT_TRUE(ind.parentesco8)</th>\n",
       "      <th>PERCENT_TRUE(ind.parentesco9)</th>\n",
       "      <th>PERCENT_TRUE(ind.v18q)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>idhogar</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>000a08204</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000bce7c4</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>001845fb0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>001ff74ca</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>003123ec2</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 148 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           hacdor  hacapo  v14a  refrig  paredblolad  paredzocalo  paredpreb  \\\n",
       "idhogar                                                                        \n",
       "000a08204   False   False  True    True         True        False      False   \n",
       "000bce7c4   False   False  True    True        False         True      False   \n",
       "001845fb0   False   False  True    True         True        False      False   \n",
       "001ff74ca   False   False  True    True         True        False      False   \n",
       "003123ec2   False   False  True    True        False        False       True   \n",
       "\n",
       "           pisocemento  pareddes  paredmad           ...            \\\n",
       "idhogar                                              ...             \n",
       "000a08204        False     False     False           ...             \n",
       "000bce7c4        False     False     False           ...             \n",
       "001845fb0        False     False     False           ...             \n",
       "001ff74ca        False     False     False           ...             \n",
       "003123ec2         True     False     False           ...             \n",
       "\n",
       "           PERCENT_TRUE(ind.parentesco12)  PERCENT_TRUE(ind.parentesco2)  \\\n",
       "idhogar                                                                    \n",
       "000a08204                             0.0                       0.333333   \n",
       "000bce7c4                             0.0                       0.500000   \n",
       "001845fb0                             0.0                       0.250000   \n",
       "001ff74ca                             0.0                       0.000000   \n",
       "003123ec2                             0.0                       0.250000   \n",
       "\n",
       "           PERCENT_TRUE(ind.parentesco3)  PERCENT_TRUE(ind.parentesco4)  \\\n",
       "idhogar                                                                   \n",
       "000a08204                       0.333333                            0.0   \n",
       "000bce7c4                       0.000000                            0.0   \n",
       "001845fb0                       0.500000                            0.0   \n",
       "001ff74ca                       0.500000                            0.0   \n",
       "003123ec2                       0.500000                            0.0   \n",
       "\n",
       "           PERCENT_TRUE(ind.parentesco5)  PERCENT_TRUE(ind.parentesco6)  \\\n",
       "idhogar                                                                   \n",
       "000a08204                            0.0                            0.0   \n",
       "000bce7c4                            0.0                            0.0   \n",
       "001845fb0                            0.0                            0.0   \n",
       "001ff74ca                            0.0                            0.0   \n",
       "003123ec2                            0.0                            0.0   \n",
       "\n",
       "           PERCENT_TRUE(ind.parentesco7)  PERCENT_TRUE(ind.parentesco8)  \\\n",
       "idhogar                                                                   \n",
       "000a08204                            0.0                            0.0   \n",
       "000bce7c4                            0.0                            0.0   \n",
       "001845fb0                            0.0                            0.0   \n",
       "001ff74ca                            0.0                            0.0   \n",
       "003123ec2                            0.0                            0.0   \n",
       "\n",
       "           PERCENT_TRUE(ind.parentesco9)  PERCENT_TRUE(ind.v18q)  \n",
       "idhogar                                                           \n",
       "000a08204                            0.0                     1.0  \n",
       "000bce7c4                            0.0                     0.0  \n",
       "001845fb0                            0.0                     0.0  \n",
       "001ff74ca                            0.0                     1.0  \n",
       "003123ec2                            0.0                     0.0  \n",
       "\n",
       "[5 rows x 148 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drop_cols = []\n",
    "for col in feature_matrix:\n",
    "    if col == 'Target':\n",
    "        pass\n",
    "    else:\n",
    "        if 'Target' in col:\n",
    "            drop_cols.append(col)\n",
    "            \n",
    "feature_matrix = feature_matrix[[x for x in feature_matrix if x not in drop_cols]]         \n",
    "feature_matrix.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "eecd9bdeaa87e0101c9012e54e2accbc52455bae"
   },
   "source": [
    "Most of these features are aggregations we could have made ourselves. However, why go to the trouble if Featuretools can do that for us?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "80a52160943fcac264829ff40240df79659e04c7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10340, 148)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "eb905e8a23c09094e7934b93b85fb20a9166d323"
   },
   "source": [
    "That one call alone gave us 147 features to train a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "760ff2c661f5df3db42006ea4352535edefff631"
   },
   "source": [
    "# Feature Selection\n",
    "\n",
    "We can do some rudimentary feature selection, removing one of any pair of columns with a correlation greater than 0.99 (absolute value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "438501339f06e3a81979d52e1668850ff1eb1638"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 7 columns with >= 0.99 correlation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['area2',\n",
       " 'tamhog',\n",
       " 'hhsize',\n",
       " 'hogar_total',\n",
       " 'MAX(ind.rez_esc)',\n",
       " 'COUNT(ind)',\n",
       " 'PERCENT_TRUE(ind.male)']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create correlation matrix\n",
    "corr_matrix = feature_matrix.corr().abs()\n",
    "# Select upper triangle of correlation matrix\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "# Find index of feature columns with correlation greater than 0.95\n",
    "to_drop = [column for column in upper.columns if any(upper[column] >= 0.99)]\n",
    "\n",
    "print('There are {} columns with >= 0.99 correlation.'.format(len(to_drop)))\n",
    "to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "9b72d2c56f7e7f91e974c459af8865607c994ef4"
   },
   "outputs": [],
   "source": [
    "feature_matrix = feature_matrix[[x for x in feature_matrix if x not in to_drop]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "06e98a366140a0c1f288c2c999f924569581aa8a"
   },
   "source": [
    "### Training and Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "eca5f1b3f93b43d225345801819b9cd186f37b65"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idhogar</th>\n",
       "      <th>hacdor</th>\n",
       "      <th>hacapo</th>\n",
       "      <th>v14a</th>\n",
       "      <th>refrig</th>\n",
       "      <th>paredblolad</th>\n",
       "      <th>paredzocalo</th>\n",
       "      <th>paredpreb</th>\n",
       "      <th>pisocemento</th>\n",
       "      <th>pareddes</th>\n",
       "      <th>...</th>\n",
       "      <th>PERCENT_TRUE(ind.parentesco12)</th>\n",
       "      <th>PERCENT_TRUE(ind.parentesco2)</th>\n",
       "      <th>PERCENT_TRUE(ind.parentesco3)</th>\n",
       "      <th>PERCENT_TRUE(ind.parentesco4)</th>\n",
       "      <th>PERCENT_TRUE(ind.parentesco5)</th>\n",
       "      <th>PERCENT_TRUE(ind.parentesco6)</th>\n",
       "      <th>PERCENT_TRUE(ind.parentesco7)</th>\n",
       "      <th>PERCENT_TRUE(ind.parentesco8)</th>\n",
       "      <th>PERCENT_TRUE(ind.parentesco9)</th>\n",
       "      <th>PERCENT_TRUE(ind.v18q)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>001ff74ca</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>003123ec2</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>004616164</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>004983866</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>005905417</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 142 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     idhogar  hacdor  hacapo  v14a  refrig  paredblolad  paredzocalo  \\\n",
       "0  001ff74ca   False   False  True    True         True        False   \n",
       "1  003123ec2   False   False  True    True        False        False   \n",
       "2  004616164   False   False  True    True        False        False   \n",
       "3  004983866   False   False  True    True        False        False   \n",
       "4  005905417   False   False  True   False        False         True   \n",
       "\n",
       "   paredpreb  pisocemento  pareddes           ...            \\\n",
       "0      False        False     False           ...             \n",
       "1       True         True     False           ...             \n",
       "2      False        False     False           ...             \n",
       "3      False         True     False           ...             \n",
       "4      False        False     False           ...             \n",
       "\n",
       "   PERCENT_TRUE(ind.parentesco12)  PERCENT_TRUE(ind.parentesco2)  \\\n",
       "0                             0.0                           0.00   \n",
       "1                             0.0                           0.25   \n",
       "2                             0.0                           0.00   \n",
       "3                             0.0                           0.00   \n",
       "4                             0.0                           0.00   \n",
       "\n",
       "   PERCENT_TRUE(ind.parentesco3)  PERCENT_TRUE(ind.parentesco4)  \\\n",
       "0                       0.500000                            0.0   \n",
       "1                       0.500000                            0.0   \n",
       "2                       0.500000                            0.0   \n",
       "3                       0.000000                            0.0   \n",
       "4                       0.666667                            0.0   \n",
       "\n",
       "   PERCENT_TRUE(ind.parentesco5)  PERCENT_TRUE(ind.parentesco6)  \\\n",
       "0                            0.0                            0.0   \n",
       "1                            0.0                            0.0   \n",
       "2                            0.0                            0.0   \n",
       "3                            0.0                            0.5   \n",
       "4                            0.0                            0.0   \n",
       "\n",
       "   PERCENT_TRUE(ind.parentesco7)  PERCENT_TRUE(ind.parentesco8)  \\\n",
       "0                            0.0                            0.0   \n",
       "1                            0.0                            0.0   \n",
       "2                            0.0                            0.0   \n",
       "3                            0.0                            0.0   \n",
       "4                            0.0                            0.0   \n",
       "\n",
       "   PERCENT_TRUE(ind.parentesco9)  PERCENT_TRUE(ind.v18q)  \n",
       "0                            0.0                     1.0  \n",
       "1                            0.0                     0.0  \n",
       "2                            0.0                     0.0  \n",
       "3                            0.0                     0.0  \n",
       "4                            0.0                     0.0  \n",
       "\n",
       "[5 rows x 142 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = feature_matrix[feature_matrix['Target'].notnull()].reset_index()\n",
    "test = feature_matrix[feature_matrix['Target'].isnull()].reset_index()\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ce1ea677e496b4befde5a3773281f43ef76541f1"
   },
   "source": [
    "# Correlations with the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "b13d3c3c1657f0a3a2728d79b7cd3a6be01ab841"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hogar_nin                        -0.268105\n",
       "r4t1                             -0.256750\n",
       "PERCENT_TRUE(ind.instlevel1)     -0.249333\n",
       "PERCENT_TRUE(ind.instlevel2)     -0.245484\n",
       "overcrowding                     -0.238148\n",
       "PERCENT_TRUE(ind.estadocivil1)   -0.215546\n",
       "eviv1                            -0.215123\n",
       "epared1                          -0.212209\n",
       "pisocemento                      -0.209285\n",
       "etecho1                          -0.205666\n",
       "Name: Target, dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrs = train.corr()\n",
    "corrs['Target'].sort_values(ascending = True).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "cb2e9a8988bc664a4c4db31027f084f3e2259052"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pisomoscer                      0.274664\n",
       "epared3                         0.282046\n",
       "PERCENT_TRUE(ind.instlevel8)    0.292295\n",
       "eviv3                           0.293213\n",
       "cielorazo                       0.293420\n",
       "MIN(ind.escolari)               0.303855\n",
       "meaneduc                        0.332315\n",
       "MAX(ind.escolari)               0.371422\n",
       "MEAN(ind.escolari)              0.420514\n",
       "Target                          1.000000\n",
       "Name: Target, dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrs['Target'].sort_values(ascending = True).dropna().tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c7baeef6f120653785471ab6025a112932d7a9aa"
   },
   "source": [
    "Featuretools has built features with moderate correlations with the `Target`. Although these correlations only show linear relationships, they can still provide an approximation of what features will be \"useful\" to a machine learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subset to Relevant Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idhogar</th>\n",
       "      <th>hacdor</th>\n",
       "      <th>hacapo</th>\n",
       "      <th>v14a</th>\n",
       "      <th>refrig</th>\n",
       "      <th>paredblolad</th>\n",
       "      <th>paredzocalo</th>\n",
       "      <th>paredpreb</th>\n",
       "      <th>pisocemento</th>\n",
       "      <th>pareddes</th>\n",
       "      <th>...</th>\n",
       "      <th>PERCENT_TRUE(ind.parentesco12)</th>\n",
       "      <th>PERCENT_TRUE(ind.parentesco2)</th>\n",
       "      <th>PERCENT_TRUE(ind.parentesco3)</th>\n",
       "      <th>PERCENT_TRUE(ind.parentesco4)</th>\n",
       "      <th>PERCENT_TRUE(ind.parentesco5)</th>\n",
       "      <th>PERCENT_TRUE(ind.parentesco6)</th>\n",
       "      <th>PERCENT_TRUE(ind.parentesco7)</th>\n",
       "      <th>PERCENT_TRUE(ind.parentesco8)</th>\n",
       "      <th>PERCENT_TRUE(ind.parentesco9)</th>\n",
       "      <th>PERCENT_TRUE(ind.v18q)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>001ff74ca</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>003123ec2</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>004616164</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>004983866</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>005905417</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 142 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     idhogar  hacdor  hacapo  v14a  refrig  paredblolad  paredzocalo  \\\n",
       "0  001ff74ca   False   False  True    True         True        False   \n",
       "1  003123ec2   False   False  True    True        False        False   \n",
       "2  004616164   False   False  True    True        False        False   \n",
       "3  004983866   False   False  True    True        False        False   \n",
       "4  005905417   False   False  True   False        False         True   \n",
       "\n",
       "   paredpreb  pisocemento  pareddes           ...            \\\n",
       "0      False        False     False           ...             \n",
       "1       True         True     False           ...             \n",
       "2      False        False     False           ...             \n",
       "3      False         True     False           ...             \n",
       "4      False        False     False           ...             \n",
       "\n",
       "   PERCENT_TRUE(ind.parentesco12)  PERCENT_TRUE(ind.parentesco2)  \\\n",
       "0                             0.0                           0.00   \n",
       "1                             0.0                           0.25   \n",
       "2                             0.0                           0.00   \n",
       "3                             0.0                           0.00   \n",
       "4                             0.0                           0.00   \n",
       "\n",
       "   PERCENT_TRUE(ind.parentesco3)  PERCENT_TRUE(ind.parentesco4)  \\\n",
       "0                       0.500000                            0.0   \n",
       "1                       0.500000                            0.0   \n",
       "2                       0.500000                            0.0   \n",
       "3                       0.000000                            0.0   \n",
       "4                       0.666667                            0.0   \n",
       "\n",
       "   PERCENT_TRUE(ind.parentesco5)  PERCENT_TRUE(ind.parentesco6)  \\\n",
       "0                            0.0                            0.0   \n",
       "1                            0.0                            0.0   \n",
       "2                            0.0                            0.0   \n",
       "3                            0.0                            0.5   \n",
       "4                            0.0                            0.0   \n",
       "\n",
       "   PERCENT_TRUE(ind.parentesco7)  PERCENT_TRUE(ind.parentesco8)  \\\n",
       "0                            0.0                            0.0   \n",
       "1                            0.0                            0.0   \n",
       "2                            0.0                            0.0   \n",
       "3                            0.0                            0.0   \n",
       "4                            0.0                            0.0   \n",
       "\n",
       "   PERCENT_TRUE(ind.parentesco9)  PERCENT_TRUE(ind.v18q)  \n",
       "0                            0.0                     1.0  \n",
       "1                            0.0                     0.0  \n",
       "2                            0.0                     0.0  \n",
       "3                            0.0                     0.0  \n",
       "4                            0.0                     0.0  \n",
       "\n",
       "[5 rows x 142 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = train[train['idhogar'].isin(list(train_valid['idhogar']))]\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idhogar</th>\n",
       "      <th>hacdor</th>\n",
       "      <th>hacapo</th>\n",
       "      <th>v14a</th>\n",
       "      <th>refrig</th>\n",
       "      <th>paredblolad</th>\n",
       "      <th>paredzocalo</th>\n",
       "      <th>paredpreb</th>\n",
       "      <th>pisocemento</th>\n",
       "      <th>pareddes</th>\n",
       "      <th>...</th>\n",
       "      <th>PERCENT_TRUE(ind.parentesco12)</th>\n",
       "      <th>PERCENT_TRUE(ind.parentesco2)</th>\n",
       "      <th>PERCENT_TRUE(ind.parentesco3)</th>\n",
       "      <th>PERCENT_TRUE(ind.parentesco4)</th>\n",
       "      <th>PERCENT_TRUE(ind.parentesco5)</th>\n",
       "      <th>PERCENT_TRUE(ind.parentesco6)</th>\n",
       "      <th>PERCENT_TRUE(ind.parentesco7)</th>\n",
       "      <th>PERCENT_TRUE(ind.parentesco8)</th>\n",
       "      <th>PERCENT_TRUE(ind.parentesco9)</th>\n",
       "      <th>PERCENT_TRUE(ind.v18q)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000a08204</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000bce7c4</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>001845fb0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>003514e22</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>003b51a87</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 142 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     idhogar  hacdor  hacapo  v14a  refrig  paredblolad  paredzocalo  \\\n",
       "0  000a08204   False   False  True    True         True        False   \n",
       "1  000bce7c4   False   False  True    True        False         True   \n",
       "2  001845fb0   False   False  True    True         True        False   \n",
       "3  003514e22   False   False  True    True        False        False   \n",
       "4  003b51a87   False   False  True    True        False        False   \n",
       "\n",
       "   paredpreb  pisocemento  pareddes           ...            \\\n",
       "0      False        False     False           ...             \n",
       "1      False        False     False           ...             \n",
       "2      False        False     False           ...             \n",
       "3       True        False     False           ...             \n",
       "4      False        False      True           ...             \n",
       "\n",
       "   PERCENT_TRUE(ind.parentesco12)  PERCENT_TRUE(ind.parentesco2)  \\\n",
       "0                             0.0                       0.333333   \n",
       "1                             0.0                       0.500000   \n",
       "2                             0.0                       0.250000   \n",
       "3                             0.0                       0.250000   \n",
       "4                             0.0                       0.000000   \n",
       "\n",
       "   PERCENT_TRUE(ind.parentesco3)  PERCENT_TRUE(ind.parentesco4)  \\\n",
       "0                       0.333333                            0.0   \n",
       "1                       0.000000                            0.0   \n",
       "2                       0.500000                            0.0   \n",
       "3                       0.500000                            0.0   \n",
       "4                       0.500000                            0.0   \n",
       "\n",
       "   PERCENT_TRUE(ind.parentesco5)  PERCENT_TRUE(ind.parentesco6)  \\\n",
       "0                            0.0                            0.0   \n",
       "1                            0.0                            0.0   \n",
       "2                            0.0                            0.0   \n",
       "3                            0.0                            0.0   \n",
       "4                            0.0                            0.0   \n",
       "\n",
       "   PERCENT_TRUE(ind.parentesco7)  PERCENT_TRUE(ind.parentesco8)  \\\n",
       "0                            0.0                            0.0   \n",
       "1                            0.0                            0.0   \n",
       "2                            0.0                            0.0   \n",
       "3                            0.0                            0.0   \n",
       "4                            0.0                            0.0   \n",
       "\n",
       "   PERCENT_TRUE(ind.parentesco9)  PERCENT_TRUE(ind.v18q)  \n",
       "0                            0.0                     1.0  \n",
       "1                            0.0                     0.0  \n",
       "2                            0.0                     0.0  \n",
       "3                            0.0                     1.0  \n",
       "4                            0.0                     0.0  \n",
       "\n",
       "[5 rows x 142 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = test[test['idhogar'].isin(list(test_valid['idhogar']))]\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "64e3ee30515f7f6b059428988b00cb4f724b5649"
   },
   "source": [
    "### Labels for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_uuid": "222c730f5611f6beed08fd78f12b8a9e0200701f"
   },
   "outputs": [],
   "source": [
    "train_labels = np.array(train.pop('Target')).reshape((-1,))\n",
    "test_ids = list(test.pop('idhogar'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2973, 140)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, test = train.align(test, axis = 1, join = 'inner')\n",
    "all_features = list(train.columns)\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0cf45f8a476a9b781d7183d51c8d28bbbbd04951"
   },
   "source": [
    "We'll now get into modeling. The gradient boosting machine implemented in LightGBM usually does well! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_uuid": "d80f9c0acfeb4fa6f39ff2f70673e8bb28619fbb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <generator object Scheduler.add_client at 0x11cefe750>\n",
      "RuntimeError: generator ignored GeneratorExit\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lightgbm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-cb5f1f3f57f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStratifiedKFold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mlightgbm\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mlgb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'lightgbm'"
     ]
    }
   ],
   "source": [
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "31775b68ae2427bd2ffa6af7daa171cdc29ae7dc"
   },
   "source": [
    "## Custom Evaluation Metric for LightGBM\n",
    "\n",
    "This is the F1 Macro score used by the competition. Defining a custom evaluation metric for Light GBM is not exactly straightforward but we can manage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "592f86efe45657e56af6e8300be86b2e9430a251"
   },
   "outputs": [],
   "source": [
    "def macro_f1_score(labels, predictions):\n",
    "    # Reshape the predictions as needed\n",
    "    predictions = predictions.reshape(len(np.unique(labels)), -1 ).argmax(axis = 0)\n",
    "    \n",
    "    metric_value = f1_score(labels, predictions, average = 'macro')\n",
    "    \n",
    "    # Return is name, value, is_higher_better\n",
    "    return 'macro_f1', metric_value, True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9e2fad73e520a69b170e50965f73be4fea660847"
   },
   "source": [
    "# Modeling with Gradient Boosting Machine\n",
    "\n",
    "The hyperparameters used here _have not been optimized_. This is meant only as a first pass at modeling with these features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "70aa9877b14ad7eeb19af201fb7748c656efd9ac"
   },
   "outputs": [],
   "source": [
    "def model_gbm(features, labels, test_features, test_ids, nfolds = 5, return_preds = False):\n",
    "    \"\"\"Model using the GBM and cross validation.\n",
    "       Trains with early stopping on each fold.\n",
    "       Hyperparameters probably need to be tuned.\"\"\"\n",
    "    \n",
    "    feature_names = list(features.columns)\n",
    "    \n",
    "    # Model with hyperparameters selected from previous work\n",
    "    model = lgb.LGBMClassifier(boosting_type = 'gbdt', n_estimators = 10000, max_depth = -1,\n",
    "                               learning_rate = 0.025, metric = 'None', min_child_samples = 30,\n",
    "                               reg_alpha = 0.35, reg_lambda = 0.6, num_leaves = 15, \n",
    "                               colsample_bytree = 0.85, objective = 'multiclass', \n",
    "                               class_weight = 'balanced', \n",
    "                               n_jobs = -1)\n",
    "    \n",
    "    # Using stratified kfold cross validation\n",
    "    strkfold = StratifiedKFold(n_splits = nfolds, shuffle = True)\n",
    "    predictions = pd.DataFrame()\n",
    "    importances = np.zeros(len(feature_names))\n",
    "    \n",
    "    # Convert to arrays for indexing\n",
    "    features = np.array(features)\n",
    "    test_features = np.array(test_features)\n",
    "    labels = np.array(labels).reshape((-1 ))\n",
    "    \n",
    "    valid_scores = []\n",
    "    \n",
    "    # Iterate through the folds\n",
    "    for i, (train_indices, valid_indices) in enumerate(strkfold.split(features, labels)):\n",
    "        # Dataframe for \n",
    "        fold_predictions = pd.DataFrame()\n",
    "        \n",
    "        # Training and validation data\n",
    "        X_train = features[train_indices]\n",
    "        X_valid = features[valid_indices]\n",
    "        y_train = labels[train_indices]\n",
    "        y_valid = labels[valid_indices]\n",
    "        \n",
    "        # Train with early stopping\n",
    "        model.fit(X_train, y_train, early_stopping_rounds = 100, \n",
    "                  eval_metric = macro_f1_score,\n",
    "                  eval_set = [(X_train, y_train), (X_valid, y_valid)],\n",
    "                  eval_names = ['train', 'valid'],\n",
    "                  verbose = 200)\n",
    "        \n",
    "        # Record the validation fold score\n",
    "        valid_scores.append(model.best_score_['valid']['macro_f1'])\n",
    "        \n",
    "        # Make predictions from the fold\n",
    "        fold_probabilitites = model.predict_proba(test_features)\n",
    "        \n",
    "        # Record each prediction for each class as a column\n",
    "        for j in range(4):\n",
    "            fold_predictions[(j + 1)] = fold_probabilitites[:, j]\n",
    "            \n",
    "        fold_predictions['idhogar'] = test_ids\n",
    "        fold_predictions['fold'] = (i+1)\n",
    "        predictions = predictions.append(fold_predictions)\n",
    "        \n",
    "        importances += model.feature_importances_ / nfolds   \n",
    "        \n",
    "        display(f'Fold {i + 1}, Validation Score: {round(valid_scores[i], 5)}, Estimators Trained: {model.best_iteration_}')\n",
    "\n",
    "    feature_importances = pd.DataFrame({'feature': feature_names,\n",
    "                                        'importance': importances})\n",
    "    valid_scores = np.array(valid_scores)\n",
    "    display(f'{nfolds} cross validation score: {round(valid_scores.mean(), 5)} with std: {round(valid_scores.std(), 5)}.')\n",
    "    \n",
    "    # If we want to examine predictions don't average over folds\n",
    "    if return_preds:\n",
    "        predictions['Target'] = predictions[[1, 2, 3, 4]].idxmax(axis = 1)\n",
    "        predictions['confidence'] = predictions[[1, 2, 3, 4]].max(axis = 1)\n",
    "        return predictions, feature_importances\n",
    "    \n",
    "    # Average the predictions over folds\n",
    "    predictions = predictions.groupby('idhogar', as_index = False).mean()\n",
    "    \n",
    "    # Find the class and associated probability\n",
    "    predictions['Target'] = predictions[[1, 2, 3, 4]].idxmax(axis = 1)\n",
    "    predictions['confidence'] = predictions[[1, 2, 3, 4]].max(axis = 1)\n",
    "    predictions = predictions.drop(columns = ['fold'])\n",
    "    \n",
    "    # Merge with the base to have one prediction for each individual\n",
    "    submission = submission_base.merge(predictions[['idhogar', 'Target']], on = 'idhogar', how = 'left').drop(columns = ['idhogar'])\n",
    "        \n",
    "    submission['Target'] = submission['Target'].fillna(4).astype(np.int8)\n",
    "    \n",
    "    # return the submission and feature importances\n",
    "    return submission, feature_importances, valid_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cb8e94be3c526e0492df7e8682c287138d531f4e"
   },
   "source": [
    "We need to make sure the length of the labels matches the length of the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "51430d3c44b62a799b71c794ae80647219161010"
   },
   "outputs": [],
   "source": [
    "len(train_labels) == train.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a6eeb2ed1b507c67a0d2a5c5932f5b28b33313d1"
   },
   "source": [
    "We should also make sure the len of `test_ids` (the `idhogar` of the testing households) is the same as the length of the testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "447a9150e2a176240a7cabd5b08e9b88e9b7200c"
   },
   "outputs": [],
   "source": [
    "len(test_ids) == test.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b378afa97001178195ff8b778caa43df61fa19db"
   },
   "source": [
    "All that's left is to model! Our first call will return the predictions themselves which are in probabilities rather than the submission dataframe. We can look at the probabilities broken down by fold to see when our model is most confident."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8819b8b87fab831613ea966405e468d35fb2887e"
   },
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "predictions, feature_importances = model_gbm(train, train_labels, test, test_ids, 5, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a4074ec370603f46bcd8810d6429de81fbaee4d6"
   },
   "outputs": [],
   "source": [
    "plt.rcParams['font.size'] = 18\n",
    "g = sns.FacetGrid(predictions, row = 'fold', hue = 'Target', size = 3, aspect = 4)\n",
    "g.map(sns.kdeplot, 'confidence');\n",
    "g.add_legend();\n",
    "plt.suptitle('Distribution of Confidence by Fold and Color', y = 1.05);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4416bc9c85ee9f723d75b57436cf98fc94fdfb3d"
   },
   "source": [
    "Our model is not very confident on any fold for any of the predictions. Overall, the class 4 seems to have the highest confidence, which makes sense because there are the most examples of this class in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c14204104a5dc31d8a6ae50542aec037a9c90bde"
   },
   "outputs": [],
   "source": [
    "predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "45fd7cc724ee4df0013e93f09cdf8f3d423adf62"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (24, 12))\n",
    "sns.violinplot(x = 'Target', y = 'confidence', hue = 'fold', data = predictions);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1910231b93a52823c915dc4c393100e3fe0a5ee2"
   },
   "source": [
    "Again, we see that class 4 has the highest confidence. There is significant difference in confidence levels across folds indications the predictions are not stable and depend heavily on the training data. Therefore, these predictions have high bias. We can potentially fix this by increasing the number of cross validation folds or by changing the sampling of the data (risky)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1f2a4606e25c06bde55298f5d8148a15bfd084c9"
   },
   "source": [
    "The next line models and returns the actual submissions for uploading. We'll also create a dataframe to keep track of the modeling results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "09b89dd670c8f2799aee6b6b7dbc16d0ca54981b"
   },
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "submission, feature_importances, valid_scores = model_gbm(train, train_labels, test, test_ids, 5)\n",
    "\n",
    "results = pd.DataFrame({'version': ['default_5fold'], \n",
    "                        'F1-mean': [valid_scores.mean()], \n",
    "                        'F1-std': [valid_scores.std()]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1e1499385e42527d829ffbd5109cff16a5581b55"
   },
   "source": [
    "I'm not running the GBM with a random seed so the same set of features can produce different cross validation results. A random seed would ensure consistent results, but may have a singificant effect on the predictions. I don't want to get caught up trying to find the \"right\" random seed so I'm letting the predictions wander for now!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bbb70e6e9b08327e3cc0e64c6f999dc665923054"
   },
   "source": [
    "## Feature Importances\n",
    "\n",
    "The utility function below plots feature importances and can show us how many features are needed for a certain cumulative level of importance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2d13fe0581e8516daf1c620c1937ec6f4bb16a79"
   },
   "outputs": [],
   "source": [
    "def plot_feature_importances(df, n = 15, return_features = False, threshold = None):\n",
    "    \"\"\"Plots n most important features. Also plots the cumulative importance if\n",
    "    threshold is specified and prints the number of features needed to reach threshold cumulative importance.\n",
    "    Intended for use with any tree-based feature importances. \n",
    "    \n",
    "    Args:\n",
    "        df (dataframe): Dataframe of feature importances. Columns must be \"feature\" and \"importance\".\n",
    "    \n",
    "        n (int): Number of most important features to plot. Default is 15.\n",
    "    \n",
    "        threshold (float): Threshold for cumulative importance plot. If not provided, no plot is made. Default is None.\n",
    "        \n",
    "    Returns:\n",
    "        df (dataframe): Dataframe ordered by feature importances with a normalized column (sums to 1) \n",
    "                        and a cumulative importance column\n",
    "    \n",
    "    Note:\n",
    "    \n",
    "        * Normalization in this case means sums to 1. \n",
    "        * Cumulative importance is calculated by summing features from most to least important\n",
    "        * A threshold of 0.9 will show the most important features needed to reach 90% of cumulative importance\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Sort features with most important at the head\n",
    "    df = df.sort_values('importance', ascending = False).reset_index(drop = True)\n",
    "    \n",
    "    # Normalize the feature importances to add up to one and calculate cumulative importance\n",
    "    df['importance_normalized'] = df['importance'] / df['importance'].sum()\n",
    "    df['cumulative_importance'] = np.cumsum(df['importance_normalized'])\n",
    "    \n",
    "    plt.rcParams['font.size'] = 12\n",
    "    plt.style.use('fivethirtyeight')\n",
    "    # Bar plot of n most important features\n",
    "    df.loc[:n, :].plot.barh(y = 'importance_normalized', \n",
    "                            x = 'feature', color = 'blue', \n",
    "                            edgecolor = 'k', figsize = (12, 8),\n",
    "                            legend = False, linewidth = 2)\n",
    "\n",
    "    plt.xlabel('Normalized Importance', size = 18); plt.ylabel(''); \n",
    "    plt.title(f'Top {n} Most Important Features', size = 18)\n",
    "    plt.gca().invert_yaxis()\n",
    "    \n",
    "    \n",
    "    if threshold:\n",
    "        # Cumulative importance plot\n",
    "        plt.figure(figsize = (8, 6))\n",
    "        plt.plot(list(range(len(df))), df['cumulative_importance'], 'b-')\n",
    "        plt.xlabel('Number of Features', size = 16); plt.ylabel('Cumulative Importance', size = 16); \n",
    "        plt.title('Cumulative Feature Importance', size = 18);\n",
    "        \n",
    "        # Number of features needed for threshold cumulative importance\n",
    "        # This is the index (will need to add 1 for the actual number)\n",
    "        importance_index = np.min(np.where(df['cumulative_importance'] > threshold))\n",
    "        \n",
    "        # Add vertical line to plot\n",
    "        plt.vlines(importance_index + 1, ymin = 0, ymax = 1.05, linestyles = '--', colors = 'red')\n",
    "        plt.show();\n",
    "        \n",
    "        print('{} features required for {:.0f}% of cumulative importance.'.format(importance_index + 1, \n",
    "                                                                                  100 * threshold))\n",
    "    if return_features:\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3a2edd2c24726e07751e1594b56632546a4374eb"
   },
   "outputs": [],
   "source": [
    "plot_feature_importances(feature_importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "aa2fe688f07bc08a783c1f5ea78bc54122bcc634"
   },
   "outputs": [],
   "source": [
    "submission.to_csv('ft_baseline.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "46c82f7824baf9788a141b7d546b60fccedb4644"
   },
   "outputs": [],
   "source": [
    "submission['Target'].value_counts().sort_index().plot.bar(color = 'blue');\n",
    "plt.title('Distribution of Predicted Labels for Individuals', size = 14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1786d5bb57505756e6918499f19a78c5308e784d"
   },
   "source": [
    "These shows the predictions on an individual, not household level (we set all individuals to 4 if they did not have a head of household). The distribution is close to what we observe in the training labels, which are provided on the household level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e307346e91592edea149fa24051a0b1eaa997d6a"
   },
   "outputs": [],
   "source": [
    "data[data['Target'].notnull()]['Target'].value_counts().sort_index().plot.bar(color = 'blue');\n",
    "plt.title('Distribution of Labels for Training Individuals', size = 12);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ca7f10e654bb5df6438feded1c3d84139ccb2122"
   },
   "source": [
    "# Custom Primitive\n",
    "\n",
    "To expand the capabilities of featuretools, we can write our own primitives to be applied to the data. We'll write a simple function that finds the range of a numeric column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0998521e75128f8ddbf82fd9f44191cb922191c3"
   },
   "outputs": [],
   "source": [
    "from featuretools.primitives import make_agg_primitive\n",
    "\n",
    "# Custom primitive\n",
    "def range_calc(numeric):\n",
    "    return np.max(numeric) - np.min(numeric)\n",
    "\n",
    "range_ = make_agg_primitive(function = range_calc,\n",
    "                            input_types = [ft.variable_types.Numeric], \n",
    "                            return_type = ft.variable_types.Numeric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "17231d1e060905541bfa9b148f0957470c88df91"
   },
   "source": [
    "We can also make a custom primitive that calculates the correlation coefficient between two columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e1b54d927a24ba1278b80cd974b07a05850c272a"
   },
   "outputs": [],
   "source": [
    "def p_corr_calc(numeric1, numeric2):\n",
    "    return np.corrcoef(numeric1, numeric2)[0, 1]\n",
    "\n",
    "pcorr_ = make_agg_primitive(function = p_corr_calc,\n",
    "                            input_types = [ft.variable_types.Numeric, ft.variable_types.Numeric], \n",
    "                            return_type = ft.variable_types.Numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train['MEAN(ind.age)'], train['MEAN(ind.escolari)'], 'o');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(train['MEAN(ind.age)'], train['MEAN(ind.escolari)'])[0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spearmanr(train['MEAN(ind.age)'], train['MEAN(ind.escolari)'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def s_corr_calc(numeric1, numeric2):\n",
    "    return spearmanr(numeric1, numeric2)[0]\n",
    "\n",
    "scorr_ = make_agg_primitive(function = s_corr_calc, \n",
    "                           input_types = [ft.variable_types.Numeric, ft.variable_types.Numeric], \n",
    "                           return_type = ft.variable_types.Numeric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f5a37e1fc09bb748548242fdaca5d0c918ef1a57"
   },
   "source": [
    "# More Featuretools\n",
    "\n",
    "Why stop with 150 features? Let's add in a few more primitives and start creating more. To prevent featuretools from building the exact same features we already have, we can add `drop_exact` and pass in the feature names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c9dc2f2fc0897ad15064b8566f34a11d39b4cbbf"
   },
   "outputs": [],
   "source": [
    "feature_matrix_add, feature_names_add = ft.dfs(entityset=es, target_entity = 'household', \n",
    "                                              agg_primitives = ['min', 'max', 'mean', 'percent_true', 'all', 'any',\n",
    "                                                             'sum', 'skew', 'std', range_],\n",
    "                                          trans_primitives = [], drop_exact = list(all_features),\n",
    "                                          max_depth = 2, \n",
    "                                          verbose = 1, n_jobs = -1, \n",
    "                                          chunk_size = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b8f0c5cb3a31a69bf8f53a9dddf1ef348d0520b2"
   },
   "outputs": [],
   "source": [
    "feature_matrix = pd.concat([feature_matrix, feature_matrix_add], axis = 1)\n",
    "feature_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a860660b6192de3d66c794e5d9afb9bd095e6eee"
   },
   "source": [
    "# Post Processing Function\n",
    "\n",
    "There are a number of steps after generating the feature matrix so let's put all of these in a function. We'll also start removing columns with more than a certain percentage of missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "04b2c2184d476050ea7cb5ded693f8f38d49f388"
   },
   "outputs": [],
   "source": [
    "def post_process(feature_matrix, all_features, missing_threshold = 0.95, correlation_threshold = 0.99):\n",
    "    \n",
    "    feature_matrix = feature_matrix.iloc[:, ~feature_matrix.columns.duplicated()]\n",
    "    feature_matrix = feature_matrix.replace({np.inf: np.nan, -np.inf:np.nan}).reset_index()\n",
    "    \n",
    "    # Remove the ids and labels\n",
    "    ids = list(feature_matrix.pop('idhogar'))\n",
    "    labels = list(feature_matrix.pop('Target'))\n",
    "    \n",
    "    # Remove columns derived from the Target\n",
    "    drop_cols = []\n",
    "    for col in feature_matrix:\n",
    "        if col == 'Target':\n",
    "            pass\n",
    "        else:\n",
    "            if 'Target' in col:\n",
    "                drop_cols.append(col)\n",
    "                \n",
    "    feature_matrix = feature_matrix[[x for x in feature_matrix if x not in drop_cols]] \n",
    "    \n",
    "    # One hot encoding (if necessary)\n",
    "    feature_matrix = pd.get_dummies(feature_matrix)\n",
    "    n_features_start = feature_matrix.shape[1]\n",
    "    print('Original shape: ', feature_matrix.shape)\n",
    "    \n",
    "    # Find missing and percentage\n",
    "    missing = pd.DataFrame(feature_matrix.isnull().sum())\n",
    "    missing['fraction'] = missing[0] / feature_matrix.shape[0]\n",
    "    missing.sort_values('fraction', ascending = False, inplace = True)\n",
    "\n",
    "    # Missing above threshold\n",
    "    missing_cols = list(missing[missing['fraction'] > missing_threshold].index)\n",
    "    n_missing_cols = len(missing_cols)\n",
    "\n",
    "    # Remove missing columns\n",
    "    feature_matrix = feature_matrix[[x for x in feature_matrix if x not in missing_cols]]\n",
    "    print('{} missing columns with threshold: {}.'.format(n_missing_cols, missing_threshold))\n",
    "    \n",
    "    # Zero variance\n",
    "    unique_counts = pd.DataFrame(feature_matrix.nunique()).sort_values(0, ascending = True)\n",
    "    zero_variance_cols = list(unique_counts[unique_counts[0] == 1].index)\n",
    "    n_zero_variance_cols = len(zero_variance_cols)\n",
    "\n",
    "    # Remove zero variance columns\n",
    "    feature_matrix = feature_matrix[[x for x in feature_matrix if x not in zero_variance_cols]]\n",
    "    print('{} zero variance columns.'.format(n_zero_variance_cols))\n",
    "    \n",
    "    # Correlations\n",
    "    corr_matrix = feature_matrix.corr()\n",
    "\n",
    "    # Extract the upper triangle of the correlation matrix\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k = 1).astype(np.bool))\n",
    "\n",
    "    # Select the features with correlations above the threshold\n",
    "    # Need to use the absolute value\n",
    "    to_drop = [column for column in upper.columns if any(upper[column].abs() > correlation_threshold)]\n",
    "\n",
    "    n_collinear = len(to_drop)\n",
    "    \n",
    "    feature_matrix = feature_matrix[[x for x in feature_matrix if x not in to_drop]]\n",
    "    print('{} collinear columns removed with correlation above {}.'.format(n_collinear,  correlation_threshold))\n",
    "    \n",
    "    total_removed = n_missing_cols + n_zero_variance_cols + n_collinear\n",
    "    \n",
    "    print('Total columns removed: ', total_removed)\n",
    "    print('Shape after feature selection: {}.'.format(feature_matrix.shape))\n",
    "    \n",
    "    # Extract the ids and labels\n",
    "    feature_matrix['idhogar'] = ids\n",
    "    feature_matrix['Target'] = labels\n",
    "    \n",
    "    # Extract out training and testing data\n",
    "    train = feature_matrix[feature_matrix['Target'].notnull()]\n",
    "    test = feature_matrix[feature_matrix['Target'].isnull()]\n",
    "    \n",
    "    # Subset to houses with a head of household\n",
    "    train = train[train['idhogar'].isin(list(train_valid['idhogar']))]\n",
    "    test = test[test['idhogar'].isin(list(test_valid['idhogar']))]\n",
    "    \n",
    "    # Training labels and testing household ids\n",
    "    train_labels = np.array(train.pop('Target')).reshape((-1,))\n",
    "    test_ids = list(test.pop('idhogar'))\n",
    "    \n",
    "    # Align the dataframes to ensure they have the same columns\n",
    "    train, test = train.align(test, join = 'inner', axis = 1)\n",
    "    \n",
    "    all_features = list(set(list(all_features) + list(train.columns)))\n",
    "    \n",
    "    assert (len(train_labels) == train.shape[0]), \"Labels must be same length as number of training observations\"\n",
    "    assert(len(test_ids) == test.shape[0]), \"Must be equal number of test ids as testing observations\"\n",
    "    \n",
    "    return train, train_labels, test, test_ids, all_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fd9bb3e7007fd0fd89e9ba3d6aaa697f24c2bb5f"
   },
   "outputs": [],
   "source": [
    "train, train_labels, test, test_ids, all_features = post_process(feature_matrix, all_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a2959894bbfb468b186dd15d37e0e0ebd117ef17"
   },
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "submission, feature_importances, valid_scores = model_gbm(train, train_labels, test, test_ids, 5)\n",
    "results = results.append(pd.DataFrame({'version': ['additional_5fold'], 'F1-mean': [valid_scores.mean()], 'F1-std': [valid_scores.std()]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a37531ee288181b9f985450aa97c06cc44d98821"
   },
   "outputs": [],
   "source": [
    "plot_feature_importances(feature_importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "06834ca261ab2b75e4417f3f2d3041bf48a58382"
   },
   "outputs": [],
   "source": [
    "submission.to_csv('more_featuretools.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_matrix_add, feature_names_add = ft.dfs(entityset=es, target_entity = 'household', \n",
    "                                       agg_primitives = ['min', 'max', 'mean', 'percent_true', 'all', 'any',\n",
    "                                                         'sum', 'skew', 'std', range_, pcorr_, scorr_],\n",
    "                                       trans_primitives = [], drop_exact = list(all_features),\n",
    "                                       max_depth = 2, max_features = 1000,\n",
    "                                       verbose = 1, n_jobs = -1, \n",
    "                                       chunk_size = 100)\n",
    "\n",
    "feature_matrix = pd.concat([feature_matrix, feature_matrix_add], axis = 1)\n",
    "feature_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "train, train_labels, test, test_ids, all_features = post_process(feature_matrix, all_features)\n",
    "submission, feature_importances, valid_scores = model_gbm(train, train_labels, test, test_ids, 5)\n",
    "results = results.append(pd.DataFrame({'version': ['additional_5fold'], 'F1-mean': [valid_scores.mean()], 'F1-std': [valid_scores.std()]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8e70435cbfda1e151c087b4318d1db222c5125d3"
   },
   "source": [
    "# Add in Divide Primitive\n",
    "\n",
    "Next we'll add a `divide` transform primitive into the deep feature synthesis call. At first we'll limit the features to 1000. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "166b6194cc9a6ea2d4b190954d7e1ebc3a6a89b7"
   },
   "outputs": [],
   "source": [
    "feature_matrix_add, feature_names_add = ft.dfs(entityset=es, target_entity = 'household', \n",
    "                                       agg_primitives = ['min', 'max', 'mean', 'percent_true', 'all', 'any',\n",
    "                                                         'sum', 'skew', 'std', range_, pcorr_],\n",
    "                                       trans_primitives = ['divide'], drop_exact = list(all_features),\n",
    "                                       max_depth = 2, max_features = 1000,\n",
    "                                       verbose = 1, n_jobs = -1, \n",
    "                                       chunk_size = 1000)\n",
    "\n",
    "feature_matrix = pd.concat([feature_matrix, feature_matrix_add], axis = 1)\n",
    "feature_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "55547ea50c03239b0515506aa49d47402c727a33"
   },
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "train, train_labels, test, test_ids, all_features = post_process(feature_matrix, all_features)\n",
    "submission, feature_importances, valid_scores = model_gbm(train, train_labels, test, test_ids, 5)\n",
    "results = results.append(pd.DataFrame({'version': ['divide1000_5fold'], 'F1-mean': [valid_scores.mean()], 'F1-std': [valid_scores.std()]}))\n",
    "submission.to_csv('divide1000_featuretools.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "257ca0f2154e1c04b32dc4766bdd0f5724ca7f61"
   },
   "outputs": [],
   "source": [
    "plot_feature_importances(feature_importances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "631fa21b19283dcc653bca4b4b95fbbbac78a964"
   },
   "source": [
    "## Increase to 1500 features\n",
    "\n",
    "1000 is clearly not enough! Most of these features are highly correlated, but we can still find useful features as evidenced by the feature importances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b1f4fb02e0908fc930145d6006d07381c483d4a2"
   },
   "outputs": [],
   "source": [
    "feature_matrix_add, feature_names_add = ft.dfs(entityset=es, target_entity = 'household', \n",
    "                                       agg_primitives = ['min', 'max', 'mean', 'percent_true', 'all', 'any',\n",
    "                                                         'sum', 'skew', 'std', range_, pcorr_],\n",
    "                                       trans_primitives = ['divide'], drop_exact = list(all_features),\n",
    "                                       max_depth = 2, max_features = 1500,\n",
    "                                       verbose = 1, n_jobs = -1, \n",
    "                                       chunk_size = 100)\n",
    "\n",
    "feature_matrix = pd.concat([feature_matrix, feature_matrix_add], axis = 1)\n",
    "feature_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "232c8ae5e9563664f9756a16894a435b38a470ec"
   },
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "train, train_labels, test, test_ids, all_features = post_process(feature_matrix, all_features)\n",
    "submission, feature_importances, valid_scores = model_gbm(train, train_labels, test, test_ids, 5)\n",
    "results = results.append(pd.DataFrame({'version': ['divide1500_5fold'], 'F1-mean': [valid_scores.mean()], 'F1-std': [valid_scores.std()]}))\n",
    "submission.to_csv('divide1500_featuretools.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "80f965bbf639c8215435414ccaae0bb8bf27784c"
   },
   "outputs": [],
   "source": [
    "plot_feature_importances(feature_importances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "eeca2bdcea18001c2c4b5a6e81aea95e52cbf6f2"
   },
   "source": [
    "## Go to 2000\n",
    "\n",
    "This is getting ridiculous.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8f32aaa7bff19ac8500e59bab0ef6d1c6ee0d2b0"
   },
   "outputs": [],
   "source": [
    "feature_matrix_add, feature_names_add = ft.dfs(entityset=es, target_entity = 'household', \n",
    "                                       agg_primitives = ['min', 'max', 'mean', 'percent_true', 'all', 'any',\n",
    "                                                         'sum', 'skew', 'std', range_, pcorr_],\n",
    "                                       trans_primitives = ['divide'], drop_exact = list(all_features),\n",
    "                                       max_depth = 2, max_features = 2000,\n",
    "                                       verbose = 1, n_jobs = -1, \n",
    "                                       chunk_size = 100)\n",
    "\n",
    "feature_matrix = pd.concat([feature_matrix, feature_matrix_add], axis = 1)\n",
    "feature_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c18f5a5bb1985f486b3c2397eedf268b94878ef7"
   },
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "\n",
    "train, train_labels, test, test_ids, all_features = post_process(feature_matrix, all_features)\n",
    "submission, feature_importances, valid_scores = model_gbm(train, train_labels, test, test_ids, 5)\n",
    "results = results.append(pd.DataFrame({'version': ['divide2000_5fold'], 'F1-mean': [valid_scores.mean()], 'F1-std': [valid_scores.std()]}))\n",
    "submission.to_csv('divide2000_featuretools.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "cdde797e836f3db34e27cb08d713358b3577c21f"
   },
   "outputs": [],
   "source": [
    "plot_feature_importances(feature_importances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "689a9b52e5918b4bbc7555d3d31c740544c40d4f"
   },
   "source": [
    "# Try Modeling with more folds\n",
    "\n",
    "As a final model, we'll increase the number of folds to 10 and see if this results in more stable predictions across folds. It's concerning that there is so much variation between folds, but that is going to happen with a small, imbalanced testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f3a4dac815dc59f7e25d942eebbe396df068272f"
   },
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "submission, feature_importances, valid_scores = model_gbm(train, train_labels, test, test_ids, 10)\n",
    "results = results.append(pd.DataFrame({'version': ['divide2000_10fold'], 'F1-mean': [valid_scores.mean()], 'F1-std': [valid_scores.std()]}))\n",
    "submission.to_csv('divide2000_10fold_featuretools.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c9e573ad4386c7a3ad53abb280315dd6d157774e"
   },
   "source": [
    "# Comparison of Models\n",
    "\n",
    "At this point we might honestly ask if there is any benefit to increasing the number of features. Only one way to find out: through data! Let's look at the performance of models so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "66ce0f0d8bc5f09f9ae750eaad85587f166d2462"
   },
   "outputs": [],
   "source": [
    "results.set_index('version', inplace = True)\n",
    "\n",
    "results['F1-mean'].plot.bar(color = 'orange', figsize = (8, 6),\n",
    "                                  yerr = list(results['F1-std']))\n",
    "plt.title('Model F1 Score Results');\n",
    "plt.ylabel('Mean F1 Score (with error bar)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4f33341a079245e380df66c988e5a98061ccb9f0"
   },
   "source": [
    "The cross validation accuracy continues to increase as we add features. I think we should be able to add more features as long as we continue to impose feature selection. The gradient boosting machine seems very good at cutting through the swath of features. Eventually we're probably going to be overfitting to the training data, but the we can address that through regularization and feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "93ff96ca2c72d0b2e90b85fd0e586fb42361ef82"
   },
   "source": [
    "# Save Data\n",
    "\n",
    "We can save the final selected featuretools feature matrix (created with a maximum of 2000 features). This will be used for Bayesian optimization of model hyperparameters. There still might be additional gains to increasing the number of features and/or using different custom primitives. My focus is now going to shift to modeling, but I encourage anyone to keep adjusting the featuretools implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_matrix = feature_matrix.iloc[:, ~feature_matrix.columns.duplicated()].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids = list(feature_matrix[(feature_matrix['Target'].notnull()) & (feature_matrix['idhogar'].isin(list(train_valid['idhogar'])))]['idhogar'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "550c72b79ffd77ed70fe82e96a5582816c32e681"
   },
   "outputs": [],
   "source": [
    "train['Target'] = train_labels\n",
    "test['Target'] = np.nan\n",
    "train['idhogar'] = train_ids\n",
    "test['idhogar'] = test_ids\n",
    "data = train.append(test)\n",
    "\n",
    "results.to_csv('model_results.csv', index = True)\n",
    "data.to_csv('ft_2000.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ae8c2f7f1b8a00a2a4fa594943e900f1557e90e8"
   },
   "source": [
    "# Conclusions\n",
    "\n",
    "Featuretools certainly can make our job easier for this problem! Adding features continues to improve the validation score with mixed effects on the public leaderboard. The next step is to optimize the model for these features. __Featuretools should be a default part of your data science workflow.__ The tool is incredibly simple to use and delivers considerable value, creating features that we never would have imagined. I look forward to seeing what the community can come up with for this problem! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
